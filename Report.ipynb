{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54b3e4b2",
   "metadata": {},
   "source": [
    "# Project Report TDT4173\n",
    "\n",
    "Kaggle Team Name: `[7] Material Girls`\n",
    "\n",
    "Team Members:\n",
    "- [564323] Eirill Bue\n",
    "- [544590] Nora Langfeldt Borgenvik\n",
    "- [586744] Silje Holm Johannesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e449284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Base path for datasets\n",
    "data_path = \"data\"\n",
    "\n",
    "# Load data\n",
    "receivals = pd.read_csv(f\"{data_path}/kernel/receivals.csv\")\n",
    "purchase_orders = pd.read_csv(f\"{data_path}/kernel/purchase_orders.csv\")\n",
    "materials = pd.read_csv(f\"{data_path}/extended/materials.csv\")\n",
    "transportation = pd.read_csv(f\"{data_path}/extended/transportation.csv\")\n",
    "\n",
    "# Convert date columns to datetime\n",
    "receivals[\"date_arrival\"] = pd.to_datetime(receivals[\"date_arrival\"], utc=True)\n",
    "purchase_orders[\"delivery_date\"] = pd.to_datetime(purchase_orders[\"delivery_date\"], utc=True)\n",
    "purchase_orders[\"created_date_time\"] = pd.to_datetime(purchase_orders[\"created_date_time\"], utc=True)\n",
    "purchase_orders[\"modified_date_time\"] = pd.to_datetime(purchase_orders[\"modified_date_time\"], utc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e79be70",
   "metadata": {},
   "source": [
    "# **Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e83aba",
   "metadata": {},
   "source": [
    "## 1. Search Domain Knowledge\n",
    "\n",
    "- Use understanding of operations to guide the analysis.  \n",
    "- Check for seasonality: are some materials delivered only at certain times?  \n",
    "- Examine supplier behavior: do certain suppliers consistently underdeliver or deliver irregularly?  \n",
    "- Consider batch splitting: how are deliveries divided into multiple batches per order?  \n",
    "\n",
    "### Overview\n",
    "\n",
    "Hydro orders materials with an expected delivery date; shipments may be transported by a carrier and are finally received at site. The receipt event records *when* the material arrived and *how much* material was actually delivered (net of packaging). This receipt event is the ground truth for arrivals and quantities.  \n",
    "\n",
    "### Data flow (real world)\n",
    "\n",
    "1. An order specifies product and quantity, with an expected delivery date.  \n",
    "2. (Sometimes) a transport leg occurs, handled by a carrier/vehicle.  \n",
    "3. One or more partial deliveries arrive; each arrival is logged with its date/time and net material weight.  \n",
    "\n",
    "### How the data links together\n",
    "\n",
    "Orders and arrivals connect through shared identifiers; one ordered item can be fulfilled by several partial deliveries. Transport records (when available) can be linked to those deliveries, and material metadata enriches the context (e.g., alloy or format).  \n",
    "\n",
    "### Timelines and measures we rely on\n",
    "\n",
    "- **Arrival date** (in UTC) marks the day the material actually counts as received.  \n",
    "- **Expected date** comes from the order and can sometimes be a placeholder (e.g., end-of-month/year).  \n",
    "- **Delay** is defined as arrival date minus expected date (positive = late, negative = early).  \n",
    "- **Fulfilment** compares total received weight to the ordered quantity for each ordered item.  \n",
    "\n",
    "### Why this matters for our forecast\n",
    "\n",
    "We forecast cumulative incoming material (net weight) over the Jan–May 2025 window using history up to end-2024. The evaluation uses a 0.2 quantile loss that penalizes overestimation much more than underestimation, so we bias features and baselines toward conservative estimates—especially where delays, split deliveries, or transport effects increase risk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7e79c2",
   "metadata": {},
   "source": [
    "## 2. Check if the Data is Intuitive\n",
    "\n",
    "The goal of this step is to verify that the data behaves logically and aligns with what we would expect from real-world raw material deliveries and purchase orders.\n",
    "We focus on detecting inconsistencies, impossible values, and structural issues that might indicate data quality problems before modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b41364a",
   "metadata": {},
   "source": [
    "### 2.1 - Check for Physically Impossible Values\n",
    "\n",
    "We start by identifying values that are not physically possible, such as negative or zero weights and quantities. Such values could indicate measurement errors, placeholder values, or incorrect data entry.\n",
    "\n",
    "We focus on the following columns from each dataset:\n",
    "\n",
    "- **Receivals:** `net_weight`\n",
    "- **Purchase Orders:** `quantity`\n",
    "- **Transportation:** `net_weight`, `gross_weight`, `tare_weight`, `vehicle_start_weight` and `vehicle_end_weight`\n",
    "- **Transportation:**  \n",
    "  `net_weight`, `gross_weight`, `tare_weight`, `vehicle_start_weight`, `vehicle_end_weight`,  \n",
    "  `wood`, `ironbands`, `plastic`, `water`, `ice`, `other`, `chips`, `packaging` and `cardboard` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db908f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count zeros and negatives per dataset\n",
    "def count_dataset_anomalies(df, columns, name):\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            num_zero = (df[col] == 0).sum()\n",
    "            num_negative = (df[col] < 0).sum()\n",
    "            total = num_zero + num_negative\n",
    "            print(f\"{col}: {total} non-positive values ({num_zero} zero, {num_negative} negative)\")\n",
    "\n",
    "# Columns to check per dataset\n",
    "receivals_cols = ['net_weight']\n",
    "purchase_orders_cols = ['quantity']\n",
    "transportation_cols = [\n",
    "    'net_weight', 'gross_weight', 'tare_weight', 'vehicle_start_weight', 'vehicle_end_weight',\n",
    "    'wood', 'ironbands', 'plastic', 'water', 'ice', 'other', 'chips', 'packaging', 'cardboard'\n",
    "]\n",
    "\n",
    "# Run anomaly counts\n",
    "count_dataset_anomalies(receivals, receivals_cols, \"Receivals\")\n",
    "count_dataset_anomalies(purchase_orders, purchase_orders_cols, \"Purchase Orders\")\n",
    "count_dataset_anomalies(transportation, transportation_cols, \"Transportation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e13e94f",
   "metadata": {},
   "source": [
    "From the output above, we can see that all examined columns have some zero or negative values. \n",
    "\n",
    "**Negative values** are very rare across the datasets. In the `Purchase Orders` dataset, a few entries have negative quantities, and in the `Transportation` dataset, only a handful of entries in other and packaging are negative. Since these negative values are so few, they can be safely removed from the datasets without affecting the analysis.\n",
    "\n",
    "**Zero values** are more frequent and likely indicate missing measurements, placeholder entries, or cases where no material was recorded. These will need to be addressed in the Data Preparation phase to ensure accurate analysis.\n",
    "\n",
    "Since `net_weight` is our target variable, we want to understand more about the zero values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffec35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_net_weight = receivals[receivals['net_weight'] == 0].copy()\n",
    "zero_net_weight['date_arrival_local'] = zero_net_weight['date_arrival'].dt.tz_convert(None)\n",
    "zero_net_weight['month'] = zero_net_weight['date_arrival_local'].dt.to_period('M')\n",
    "monthly_counts = zero_net_weight.groupby('month').size().reset_index(name='count')\n",
    "\n",
    "plt.figure(figsize=(16,5))\n",
    "sns.barplot(\n",
    "    data=monthly_counts,\n",
    "    x='month',\n",
    "    y='count',\n",
    "    color='darkorange'\n",
    ")\n",
    "plt.title('Zero net_weight entries per month', weight='bold', fontsize=16)\n",
    "plt.xlabel('Month', fontsize=14)\n",
    "plt.ylabel('Count of Zero net_weight', fontsize=14)\n",
    "plt.xticks(rotation=90, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "for i, v in enumerate(monthly_counts['count']):\n",
    "    plt.text(i, v + max(monthly_counts['count'])*0.01, str(v), ha='center', va='bottom', fontsize=12, weight='bold')\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5067df35",
   "metadata": {},
   "source": [
    "We plot the zero `net_weight` entries aggregated by month to understand their temporal distribution. While there are 137 zero values in total, they are relatively evenly distributed across the years, with a slight concentration in July 2013. Since `net_weight` is our target variable, these entries would provide incorrect information for predicting it and should therefore be removed. Keeping them would likely degrade prediction quality and introduce noise into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81e75a1",
   "metadata": {},
   "source": [
    "### 2.2 - Check Unit Consistency\n",
    "\n",
    "Next, we verify that all units are expressed consistently across records.\n",
    "For example, we ensure that all weights use the same measurement unit (e.g., kilograms) and that no mix of units (such as kg, ton, or g) exists.\n",
    "We will also check for missing or malformed unit entries and decide whether these records can be trusted.\n",
    "\n",
    "The `purchase_orders` table contains the unit information (`unit` and `unit_id`) for all orders. To check if all entries are expressed consistently:\n",
    "\n",
    "- We count how many entries there are for each unit.  \n",
    "- We check if any `unit_id` maps to more than one unit, which would indicate inconsistent unit assignments.  \n",
    "- We check if any `product_id` has more that one unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b32be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_counts = purchase_orders['unit'].value_counts()\n",
    "print(\"Number of entries per unit:\")\n",
    "print(unit_counts)\n",
    "\n",
    "unit_inconsistency = purchase_orders.groupby('unit_id')['unit'].nunique()\n",
    "num_inconsistent_unit_ids = (unit_inconsistency > 1).sum()\n",
    "print(\"\\nNumber of unit_ids mapping to more than one unit:\", num_inconsistent_unit_ids)\n",
    "\n",
    "units_per_product = purchase_orders.groupby('product_id')['unit'].unique()\n",
    "products_multiple_units = units_per_product[units_per_product.apply(lambda x: len(x) > 1)]\n",
    "print(\"\\nProducts with more than one unit type:\")\n",
    "print(products_multiple_units)\n",
    "\n",
    "prod_1414_units = purchase_orders[purchase_orders['product_id'] == 1414.0]['unit'].value_counts()\n",
    "print(\"\\nNumber of entries per unit for product 1414.0:\")\n",
    "print(prod_1414_units)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5309f604",
   "metadata": {},
   "source": [
    "As we can see from the data, almost all entries are in KG, and only a few entries are in PUND.  \n",
    "\n",
    "A few products have missing units (NaN), but only `product_id` 1414 has entries in both KG and PUND. In the data preparation, we will convert all quantities to a consistent unit, kilograms.  This ensures that all quantities are comparable and consistent for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9abbb7",
   "metadata": {},
   "source": [
    "### 2.3 - Check Temporal Consistency\n",
    "\n",
    "We then confirm that all date fields follow logical order and fall within reasonable time frames. This involves checking that:\n",
    "\n",
    "- Created purchase order dates are not after arrival dates.\n",
    "- No timestamps are in the future (after 31st of December 2024).\n",
    "- Date formats and time zones are consistent across tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9151077",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = receivals.merge(\n",
    "    purchase_orders[['purchase_order_id', 'purchase_order_item_no', 'created_date_time']],\n",
    "    on=['purchase_order_id', 'purchase_order_item_no'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Check if arrival happens before purchase order was created\n",
    "arrival_before_creation = merged[merged['date_arrival'] < merged['created_date_time']]\n",
    "print(\"Number of receivals arriving before purchase order creation:\", len(arrival_before_creation))\n",
    "\n",
    "# Check for future dates after 31st December 2024\n",
    "cutoff_date = pd.Timestamp(\"2024-12-31\", tz=\"UTC\")\n",
    "\n",
    "receivals_future = receivals[receivals['date_arrival'] > cutoff_date]\n",
    "po_future_created = purchase_orders[purchase_orders['created_date_time'] > cutoff_date]\n",
    "\n",
    "print(\"Receivals - entries with date_arrival after 2024-12-31:\", len(receivals_future))\n",
    "print(\"Purchase Orders - created_date_time after 2024-12-31:\", len(po_future_created))\n",
    "\n",
    "\n",
    "# Total number of receivals\n",
    "total_receivals = len(receivals)\n",
    "print(\"\\nTotal number of receivals:\", total_receivals)\n",
    "\n",
    "# Number of receivals arriving before purchase order creation\n",
    "arrival_before_creation = merged[merged['date_arrival'] < merged['created_date_time']]\n",
    "num_before_creation = len(arrival_before_creation)\n",
    "print(\"Number of receivals arriving before purchase order creation:\", num_before_creation)\n",
    "\n",
    "# Percentage\n",
    "percent = num_before_creation / total_receivals * 100\n",
    "print(f\"Percentage of receivals affected: {percent:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91704618",
   "metadata": {},
   "source": [
    "We examined the temporal consistency of the datasets to ensure that date fields follow a logical order and remain within a valid time range. No receivals or purchase orders have timestamps beyond 31st December 2024, meaning the data does not contain future or implausible dates.\n",
    "\n",
    "Furthermore, out of all receivals, 3.83% of them have an arrival date that occurs before the corresponding purchase order was created. This indicates a small number of inconsistencies where the recorded creation timestamp likely does not reflect the actual order initiation date. To better understand the temporal inconsistencies identified, we visualize the affected receivals by year, as well as the top products and suppliers involved. These plots will help highlight patterns and guide further investigation into the underlying causes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d284f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "merged = receivals.merge(\n",
    "    purchase_orders[['purchase_order_id', 'purchase_order_item_no', 'created_date_time']],\n",
    "    on=['purchase_order_id', 'purchase_order_item_no'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "inconsistent = merged[merged['date_arrival'] < merged['created_date_time']].copy()\n",
    "inconsistent.loc[:, \"arrival_year\"] = inconsistent[\"date_arrival\"].dt.year\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", context=\"talk\", font=\"DejaVu Sans\")\n",
    "plt.rcParams.update({'axes.titlesize': 16, 'axes.labelsize': 14, 'xtick.labelsize': 12, 'ytick.labelsize': 12})\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = GridSpec(2, 2, height_ratios=[2, 3], width_ratios=[1, 1], figure=fig)\n",
    "\n",
    "# Graph 1\n",
    "ax0 = fig.add_subplot(gs[0, :])\n",
    "year_counts = inconsistent[\"arrival_year\"].value_counts().sort_index()\n",
    "sns.barplot(\n",
    "    x=year_counts.index,\n",
    "    y=year_counts.values,\n",
    "    color=\"#87CEEB\",  # skyblue\n",
    "    ax=ax0\n",
    ")\n",
    "ax0.set_title(\"Inconsistent Receivals by Year\", weight=\"bold\")\n",
    "ax0.set_xlabel(\"Arrival Year\")\n",
    "ax0.set_ylabel(\"Number of Inconsistencies\")\n",
    "ax0.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "for i, v in enumerate(year_counts.values):\n",
    "    ax0.text(i, v + max(year_counts.values)*0.01, str(v), ha='center', va='bottom', fontsize=12, weight='bold')\n",
    "\n",
    "# Graph 2\n",
    "top_products = inconsistent[\"product_id\"].value_counts().head(10).reset_index()\n",
    "top_products.columns = [\"product_id\", \"count\"]\n",
    "top_products[\"count\"] = top_products[\"count\"].astype(int)\n",
    "\n",
    "ax1 = fig.add_subplot(gs[1, 0])\n",
    "sns.barplot(\n",
    "    data=top_products,\n",
    "    x=\"product_id\",\n",
    "    y=\"count\",\n",
    "    color=\"#89a978\",\n",
    "    ax=ax1\n",
    ")\n",
    "ax1.set_title(\"Top 10 Products with Temporal Inconsistencies\", weight=\"bold\")\n",
    "ax1.set_xlabel(\"Product ID\")\n",
    "ax1.set_ylabel(\"Count of Inconsistencies\")\n",
    "ax1.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "for p in ax1.patches:\n",
    "    ax1.annotate(\n",
    "        f'{int(p.get_height())}',\n",
    "        (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "        ha='center',\n",
    "        va='bottom',\n",
    "        fontsize=12,\n",
    "        weight='bold'\n",
    "    )\n",
    "\n",
    "# Graph 3\n",
    "top_suppliers = inconsistent[\"supplier_id\"].value_counts().head(10).reset_index()\n",
    "top_suppliers.columns = [\"supplier_id\", \"count\"]\n",
    "top_suppliers[\"count\"] = top_suppliers[\"count\"].astype(int)\n",
    "\n",
    "ax2 = fig.add_subplot(gs[1, 1])\n",
    "sns.barplot(\n",
    "    data=top_suppliers,\n",
    "    x=\"supplier_id\",\n",
    "    y=\"count\",\n",
    "    color=\"#ff7f0e\",\n",
    "    ax=ax2\n",
    ")\n",
    "ax2.set_title(\"Top 10 Suppliers with Temporal Inconsistencies\", weight=\"bold\")\n",
    "ax2.set_xlabel(\"Supplier ID\")\n",
    "ax2.set_ylabel(\"Count of Inconsistencies\")\n",
    "ax2.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "for p in ax2.patches:\n",
    "    ax2.annotate(\n",
    "        f'{int(p.get_height())}',\n",
    "        (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "        ha='center',\n",
    "        va='bottom',\n",
    "        fontsize=12,\n",
    "        weight='bold'\n",
    "    )\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0efe77",
   "metadata": {},
   "source": [
    "To handle these issues, all inconsistent receivals will be flagged during the data preparation phase. We will investigate whether the recorded arrival or creation dates are incorrect and either correct them if possible or exclude the affected entries from further analysis to ensure temporal integrity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7bb4fc",
   "metadata": {},
   "source": [
    "### 2.4 - Check ID Consistency and Structure\n",
    "\n",
    "We ensure that all identifiers, such as purchase_order_id, product_id, and supplier_id, are positive integers and behave consistently over time.\n",
    "This includes checking whether any IDs appear to change unexpectedly, for example, if a material or supplier seems to get a new ID from one year to the next.\n",
    "Such cases could indicate changes in the underlying data system or joining keys that require standardization later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f796e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work on copies to avoid modifying original data\n",
    "rec = receivals.copy()\n",
    "po = purchase_orders.copy()\n",
    "mat = materials.copy()\n",
    "trans = transportation.copy()\n",
    "\n",
    "# Define ID columns per table\n",
    "id_columns = {\n",
    "    \"receivals\": [\"rm_id\", \"product_id\", \"purchase_order_id\", \"purchase_order_item_no\", \"batch_id\", \"receival_item_no\", \"supplier_id\"],\n",
    "    \"purchase_orders\": [\"purchase_order_id\", \"purchase_order_item_no\", \"product_id\", \"product_version\", \"unit_id\", \"status_id\"],\n",
    "    \"materials\": [\"rm_id\", \"product_id\", \"product_version\"],\n",
    "    \"transportation\": [\"rm_id\", \"product_id\", \"purchase_order_id\", \"purchase_order_item_no\", \"receival_item_no\", \"batch_id\"]\n",
    "}\n",
    "\n",
    "# Check for negative IDs\n",
    "for table_name, cols in id_columns.items():\n",
    "    print(f\"\\nTable: {table_name}\")\n",
    "    df = locals()[table_name]\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            negative_ids = df[df[col] < 0]\n",
    "            print(f\"{col}: {len(negative_ids)} negative entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3912d26",
   "metadata": {},
   "source": [
    "All ID fields across the four datasets (receivals, purchase_orders, materials, and transportation) contain only positive values. This suggests that the identifier structure is valid and no corrupted or placeholder IDs.\n",
    "\n",
    "However, while the IDs are numerically valid, there may still be semantic inconsistencies. For example, if a material (identified by rm_id) appears to have been assigned a new ID in later years, or if the same material name or description maps to multiple IDs. Such changes could indicate updates or restructuring in the underlying system.\n",
    "\n",
    "To explore this, we can check whether materials with the same rm_id are associated with different names over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023f57af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Merge receivals with material info to connect rm_id and raw_material_alloy\n",
    "merged_materials = receivals.merge(\n",
    "    materials[['rm_id', 'product_id', 'raw_material_alloy']],\n",
    "    on=['rm_id', 'product_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 1. Number of raw materials with multiple rm_ids\n",
    "multiple_ids_per_alloy = (\n",
    "    merged_materials.groupby('raw_material_alloy')['rm_id']\n",
    "    .nunique()\n",
    "    .loc[lambda x: x > 1]\n",
    ")\n",
    "num_raw_materials_multiple_rmids = multiple_ids_per_alloy.shape[0]\n",
    "print(\"Number of raw materials with multiple rm_ids:\", num_raw_materials_multiple_rmids)\n",
    "\n",
    "# 2. Number of rm_ids linked to multiple raw material names\n",
    "multiple_alloys_per_rm = (\n",
    "    merged_materials.groupby('rm_id')['raw_material_alloy']\n",
    "    .nunique()\n",
    "    .loc[lambda x: x > 1]\n",
    ")\n",
    "num_rmids_multiple_alloys = multiple_alloys_per_rm.shape[0]\n",
    "print(\"Number of rm_ids linked to multiple alloys:\", num_rmids_multiple_alloys)\n",
    "\n",
    "# 3. Prepare summary table for all rm_id → alloys (flag only if multiple alloys)\n",
    "summary = []\n",
    "for rm_id in merged_materials['rm_id'].unique():\n",
    "    subset = merged_materials[merged_materials['rm_id'] == rm_id]\n",
    "    alloys = subset['raw_material_alloy'].unique()\n",
    "    \n",
    "    # Flag only if more than one alloy\n",
    "    if len(alloys) > 1:\n",
    "        # Check if all alloys have identical arrival dates\n",
    "        timeline = subset.groupby('raw_material_alloy')['date_arrival'].agg(['min', 'max'])\n",
    "        same_dates = timeline.nunique().eq(1).all()\n",
    "        if same_dates:\n",
    "            flag = 'naming inconsistency'\n",
    "        else:\n",
    "            flag = 'possible real change'\n",
    "    else:\n",
    "        flag = None\n",
    "\n",
    "    summary.append({\n",
    "        'rm_id': rm_id,\n",
    "        'alloys': list(alloys),\n",
    "        'flag': flag,\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "\n",
    "# Only display rm_ids linked to multiple alloys with a flag\n",
    "multi_alloy_summary = summary_df[summary_df['flag'].notna()]\n",
    "\n",
    "display(multi_alloy_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f1b866",
   "metadata": {},
   "source": [
    "We analyzed the relationship between raw materials (`rm_id`s) and their alloy names (`raw_material_alloy`) to detect inconsistencies or potential changes in material usage.\n",
    "\n",
    "- Alloys linked to multiple `rm_id`s were counted to detect duplicates recorded under different IDs.\n",
    "- `rm_id`s linked with multiple alloy names were flagged for potential naming inconsistencies.\n",
    "- For `rm_id`s with multiple alloy names, we compared the arrival dates:\n",
    "   - If the date ranges overlapped, it suggests the same material was recorded with different names. This is flagged as **naming inconsistency**.\n",
    "   - If the date ranges did not overlap, it suggests a possible material change over time. This is flagged as **possible real change**.\n",
    "- We compiled a summary table showing each `rm_id`, its associated alloy names, and the corresponding flag. This table allows us to quickly identify which materials have naming issues and which might represent actual changes.\n",
    "\n",
    "\n",
    "**Findings:**\n",
    "\n",
    "- **Raw materials linked to multiple `rm_id`s:** 34  \n",
    "- **`rm_id`s linked to multiple alloy names:** 24  \n",
    "- Among these, most were naming inconsistencies, but one `rm_id` stood out as a **possible real change**, indicating that the material might have genuinely changed over time.\n",
    "\n",
    "\n",
    "**Next steps:**\n",
    "\n",
    "- For the possible real change, we will update the record to a single alloy for that `rm_id`.\n",
    "- For naming inconsistencies, we will keep the `rm_id`s separate for now, rather than standardizing the alloy names. This is important because predictions are made per `rm_id`, and merging or renaming would make the predictions identical for different `rm_id`s, which could distort the results. We should, however, document the inconsistencies for reference and future review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f545506",
   "metadata": {},
   "outputs": [],
   "source": [
    "alloy_counts = materials.groupby('raw_material_alloy')['rm_id'].nunique()\n",
    "top_alloy = alloy_counts.idxmax()\n",
    "\n",
    "top_alloy_rm_ids = materials[materials['raw_material_alloy'] == top_alloy]['rm_id'].unique()\n",
    "\n",
    "pm_data = receivals[receivals['rm_id'].isin(top_alloy_rm_ids)]\n",
    "\n",
    "summary = pm_data.groupby('rm_id').agg(\n",
    "    min_date=('date_arrival', 'min'),\n",
    "    max_date=('date_arrival', 'max'),\n",
    "    count_receivals=('receival_item_no', 'count'),\n",
    "    avg_net_weight=('net_weight', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "summary['alloy_name'] = top_alloy\n",
    "\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2398f189",
   "metadata": {},
   "source": [
    "We also computed a table for an example alloy associated with several rm_ids to examine their receival histories. From this table, we can see that:\n",
    "\n",
    "- The different rm_ids have varying date ranges and numbers of receivals.\n",
    "- The average net weights are quite similar across rm_ids, indicating that the material itself is consistent.\n",
    "\n",
    "This suggests that we might be able to aggregate these receivals for prediction purposes, as the target variable is comparable despite differences in timing and number of entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840ab5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge receivals with materials to get alloy info\n",
    "receivals_with_alloy = receivals.merge(\n",
    "    materials[['rm_id', 'raw_material_alloy']],\n",
    "    on='rm_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 1. Count receivals per rm_id\n",
    "receival_counts = receivals_with_alloy.groupby('rm_id').agg(\n",
    "    count_receivals=('receival_item_no', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# 2. Count unique alloys per rm_id\n",
    "alloys_per_rm = receivals_with_alloy.groupby('rm_id')['raw_material_alloy'].nunique().reset_index()\n",
    "alloys_per_rm = alloys_per_rm.rename(columns={'raw_material_alloy': 'num_alloys'})\n",
    "\n",
    "# 3. Get the actual alloy name(s) for each rm_id\n",
    "alloy_names = receivals_with_alloy.groupby('rm_id')['raw_material_alloy'].unique().reset_index()\n",
    "\n",
    "# 4. Count how many rm_ids share each alloy\n",
    "alloy_rm_counts = receivals_with_alloy.groupby('raw_material_alloy')['rm_id'].nunique().reset_index()\n",
    "alloy_rm_counts = alloy_rm_counts.rename(columns={'rm_id': 'num_rm_ids'})\n",
    "\n",
    "# 5. Merge everything together\n",
    "low_count_table = receival_counts.merge(alloys_per_rm, on='rm_id') \\\n",
    "                                 .merge(alloy_names, on='rm_id') \\\n",
    "                                 .explode('raw_material_alloy') \\\n",
    "                                 .merge(alloy_rm_counts, on='raw_material_alloy')\n",
    "\n",
    "# 6. Filter for rm_ids with fewer than 2 receivals\n",
    "low_count_table = low_count_table[low_count_table['count_receivals'] < 2]\n",
    "\n",
    "# Display the full table\n",
    "display(low_count_table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cad3c3",
   "metadata": {},
   "source": [
    "In this table, we focus on rm_ids with only one receival entry. For each of these:\n",
    "\n",
    "- We check the associated alloy name.\n",
    "- We check if the alloy name appears across multiple rm_ids.\n",
    "\n",
    "If it does, it suggests these rm_ids represent the same material, and we could potentially use their receivals to get more data points for predictions.\n",
    "\n",
    "If the alloy name is unique to that rm_id, there is no additional data to leverage, which poses a challenge for prediction since we have only a single receival for that material stream. This makes the prediction less reliable and highlights the limitation of working with very sparse data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948e12b8",
   "metadata": {},
   "source": [
    "### 2.5 - Summarize Actions for Data Preparation\n",
    "\n",
    "Finally, we summarize the issues identified in this step and outline the cleaning actions that will be performed during the Data Preparation phase.\n",
    "\n",
    "1. **Physically impossible values**  \n",
    "   - Negative values in weights and quantities are rare and can be safely removed.  \n",
    "   - Zero values are more frequent, particularly in `net_weight`, which is the target variable. Since these entries provide incorrect information for predictions, they should be removed to avoid introducing noise and degrading model performance.\n",
    "\n",
    "2. **Unit consistency**  \n",
    "   - Units are mostly consistent, with almost all entries in kilograms.  \n",
    "   - A few entries use PUND or have missing units. These will be converted to a consistent unit (kilograms) to ensure comparability across all records.\n",
    "\n",
    "3. **Temporal consistency**  \n",
    "   - A small percentage of receivals occur before the corresponding purchase order was created, indicating minor temporal inconsistencies.  \n",
    "   - No dates exceed the current time frame.  \n",
    "   - **Action:** Flag inconsistent entries for further investigation. Correct or exclude them to maintain temporal integrity.\n",
    "\n",
    "4. **ID consistency and structure**  \n",
    "   - All identifier fields contain positive integers, suggesting no corrupted IDs.  \n",
    "   - Semantic inconsistencies may exist, e.g., a single material assigned multiple `rm_id`s or multiple alloy names per `rm_id`.  \n",
    "   - **Action:** Document naming inconsistencies and possible material changes. For possible real changes, standardize the alloy name for that `rm_id`. For naming inconsistencies, retain separate `rm_id`s to preserve prediction integrity.\n",
    "\n",
    "5. **Materials linked to multiple `rm_id`s**  \n",
    "   - Some alloys are associated with multiple `rm_id`s. Although receival dates and counts vary, the **average net weight is generally similar**, indicating the material itself is consistent.  \n",
    "   - **Action:** When the target variable is comparable, we may aggregate data across `rm_id`s for predictions, increasing the number of usable data points.\n",
    "\n",
    "6. **`rm_id`s with only a single receival**  \n",
    "   - For each of these, we check the associated alloy name and whether it appears for multiple `rm_id`s.  \n",
    "   - If the alloy appears across multiple `rm_id`s, we can leverage those additional receivals for predictions.  \n",
    "   - If the alloy is unique to that `rm_id`, there is only a single receival available. This sparse data makes predictions less reliable and may require special handling, such as exclusion or careful imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d2cee0",
   "metadata": {},
   "source": [
    "## 3. Understand How the Data Was Generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea811ef8",
   "metadata": {},
   "source": [
    "### Receivals key uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c60164",
   "metadata": {},
   "outputs": [],
   "source": [
    "recv = receivals.copy()         \n",
    "\n",
    "key_sets = [\n",
    "    [\"purchase_order_id\", \"purchase_order_item_no\", \"receival_item_no\"],\n",
    "    [\"batch_id\", \"receival_item_no\"]\n",
    "]\n",
    "for keys in key_sets:\n",
    "    dup = recv.groupby(keys).size().gt(1).sum()\n",
    "    total = recv.groupby(keys).ngroups\n",
    "    print(f\"{keys}: {dup} duplicate groups of {total} total ({dup/total:.2%} duplicates)\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c155c2",
   "metadata": {},
   "source": [
    "Receival_item_no is not unique within a purchase order item. It likely resets per batch, or is reused when a purchase order item is split across batches/lines.\n",
    "\n",
    "The pair [batch_id, receival_item_no] is unique. So receival_item_no seems to be scoped to the batch, not to the purchase order item. Use [batch_id, receival_item_no] as the primary key when aggregating or joining receivals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574f4c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_bad = [\"purchase_order_id\",\"purchase_order_item_no\",\"receival_item_no\"]\n",
    "dups = recv[recv.duplicated(keys_bad, keep=False)] \\\n",
    "        .sort_values(keys_bad)\n",
    "cols = keys_bad + [\"batch_id\",\"date_arrival\",\"rm_id\",\"product_id\",\"net_weight\",\"receival_status\"]\n",
    "dups[cols].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcf99f2",
   "metadata": {},
   "source": [
    "Looking at sample rows, those duplicates have different arrival dates and/or different net weights. In other words, they’re partial deliveries for the same purchase order line, not accidental double entries.\n",
    "\n",
    "Aggregate daily totals by the calendar day of date_arrival and by a material identifier (rm_id or product_id).\n",
    "For a unique receival key, prefer [batch_id, receival_item_no]. If batch_id is missing, use [purchase_order_id, purchase_order_item_no, receival_item_no, date_arrival], and append a within-group sequence if duplicates remain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1c0db8",
   "metadata": {},
   "source": [
    "### Link receivals and purchase orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738e9ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "recv2 = receivals.copy()\n",
    "po = purchase_orders.copy()    \n",
    "\n",
    "join_cols = [\"purchase_order_id\", \"purchase_order_item_no\"]\n",
    "merged = recv2.merge(po[join_cols + [\"quantity\",\"delivery_date\"]], on=join_cols, how=\"left\")\n",
    "\n",
    "tot   = len(merged)\n",
    "unmatched = merged[\"quantity\"].isna().sum()\n",
    "matched   = tot - unmatched\n",
    "\n",
    "print(f\"Matched:   {matched:,}\")\n",
    "print(f\"Unmatched: {unmatched:,}\")\n",
    "print(f\"Total:     {tot:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000e31b6",
   "metadata": {},
   "source": [
    "56 receival rows is not linked to a purchase order line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f8433f",
   "metadata": {},
   "source": [
    "### Planned vs actual delivery lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df993e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "recv3 = receivals.copy()\n",
    "po2 = purchase_orders.copy()    \n",
    "\n",
    "lag_df = recv3.merge(po2[join_cols + [\"delivery_date\"]], on=join_cols, how=\"left\")\n",
    "lag = (lag_df[\"date_arrival\"] - lag_df[\"delivery_date\"]).dt.days.dropna()\n",
    "if len(lag):\n",
    "    print(f\"Count={len(lag):,}, Median={lag.median():.1f}, p25={lag.quantile(0.25):.1f}, p75={lag.quantile(0.75):.1f}, p95={lag.quantile(0.95):.1f}\")\n",
    "    print(f\"Late={100*(lag>0).mean():.1f}%, Early={100*(lag<0).mean():.1f}%, On-time={100*(lag==0).mean():.1f}%\")\n",
    "else:\n",
    "    print(\"Not enough valid dates.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d9a11b",
   "metadata": {},
   "source": [
    "The majority of the arrival date beat the expected date by 1–4 weeks, and very few land exactly on it.\n",
    "Late receipts are a minority (≈8%), and the late tail isn’t big (95% are ≤9 days late).\n",
    "\n",
    "If you use delivery_date for service-level KPIs, consider classifying anything within a negative window (e.g., up to 2 weeks early) as on-time, or recalibrating targets by supplier/material so the metric reflects reality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ff3ac2",
   "metadata": {},
   "source": [
    "### Purchase order date quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10332e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "po3 = purchase_orders.copy()  \n",
    "\n",
    "month_end = po3[\"delivery_date\"].dt.is_month_end.mean()\n",
    "print(f\"delivery_date at month-end (placeholder risk): {month_end:.1%}\")\n",
    "\n",
    "ok = (po3[\"modified_date_time\"] >= po3[\"created_date_time\"]).mean()\n",
    "print(f\"modified_date_time ≥ created_date_time: {ok:.1%}\")\n",
    "\n",
    "# only rows were both dates exist\n",
    "valid = po3[\"modified_date_time\"].notna() & po3[\"created_date_time\"].notna()\n",
    "\n",
    "ok_count   = (po3.loc[valid, \"modified_date_time\"] >= po3.loc[valid, \"created_date_time\"]).sum()\n",
    "total_val  = valid.sum()\n",
    "bad_count  = total_val - ok_count         \n",
    "missing    = len(po3) - total_val            \n",
    "\n",
    "print(f\"Modified ≥ created): {ok_count:,} av {total_val:,} \"\n",
    "      f\"({ok_count/total_val:.1%})\")\n",
    "print(f\"Modified < created): {bad_count:,}\")\n",
    "print(f\"Missing dates: {missing:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae34d60",
   "metadata": {},
   "source": [
    "We flagged delivery dates that fall exactly on the last day of a month—a common placeholder when someone doesn’t know the real date. Only 0.5% of purchase order lines have a month-end date, so the risk of placeholder delivery dates is very low.\n",
    "\n",
    "The audit trail is internally consistent wherever timestamps are present. The only quality gap is coverage, some purchase orders don’t have both timestamps. Cleaning or backfilling those 492 records would likely lift the overall pass rate from 98.5% to 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf37f17b",
   "metadata": {},
   "source": [
    "## 4. Explore Individual Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda962a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_feature_summary(df, name, top_n=20):\n",
    "\n",
    "    print(f\"\\n[{name}] feature summary\")\n",
    "    print(\"-\" * (len(name) + 20))\n",
    "    \n",
    "    rows = []\n",
    "    for c in df.columns:\n",
    "        s = df[c]\n",
    "        dtype = str(s.dtype)\n",
    "        info = {\n",
    "            \"column\": c,\n",
    "            \"dtype\": dtype,\n",
    "            \"non_null%\": (1 - s.isna().mean()) * 100,\n",
    "            \"nunique\": s.nunique(dropna=True)\n",
    "        }\n",
    "\n",
    "        # Handle numeric\n",
    "        if pd.api.types.is_numeric_dtype(s):\n",
    "            x = pd.to_numeric(s, errors=\"coerce\")\n",
    "            info.update(dict(\n",
    "                min=np.nanmin(x), \n",
    "                q25=np.nanquantile(x, .25), \n",
    "                median=np.nanquantile(x, .5),\n",
    "                q75=np.nanquantile(x, .75), \n",
    "                max=np.nanmax(x), \n",
    "                mean=np.nanmean(x),\n",
    "                std=np.nanstd(x), \n",
    "                skew=pd.Series(x).skew(skipna=True)\n",
    "            ))\n",
    "\n",
    "        # Handle datetimes\n",
    "        elif pd.api.types.is_datetime64_any_dtype(s):\n",
    "            info.update(dict(\n",
    "                min_ts=s.min(), \n",
    "                max_ts=s.max()))\n",
    "\n",
    "        # Treat everything else as categorical/text\n",
    "        else:\n",
    "            ex = s.dropna().astype(str).unique()[:5]\n",
    "            info[\"examples\"] = \", \".join(ex)\n",
    "\n",
    "        rows.append(info)\n",
    "\n",
    "    table = pd.DataFrame(rows)\n",
    "    print(table.head(top_n).to_string(index=False))\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cb6abf",
   "metadata": {},
   "source": [
    "### Receivals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabc8165",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_recv = preview_feature_summary(recv, \"Receivals\", top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375f4041",
   "metadata": {},
   "source": [
    "Half of the rows have no batch; don’t rely on batch_id alone as a key. Net_weight has a minimum value of 0.0 (meaning some rows are missing), skew is around +0.07, which is nearly symmetric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b836638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "recv4 = receivals.copy()\n",
    "\n",
    "# missing net_weight\n",
    "miss = recv4['net_weight'].isna().sum()\n",
    "print(f\"Missing net_weight: {miss:,} / {len(recv4):,} \")\n",
    "\n",
    "# outliers\n",
    "x = recv4[\"net_weight\"]\n",
    "q1, q3 = x.quantile([.25, .75])\n",
    "iqr = q3 - q1\n",
    "lo, hi = q1 - 0.5*iqr, q3 + 0.5*iqr  # at 0.75*iqr there were 0 outliers\n",
    "\n",
    "mask_iqr = (x < lo) | (x > hi)           \n",
    "print(f\"IQR outliers: {mask_iqr.sum():,} / {len(recv4):,}  (q1={q1:.1f}, q3={q3:.1f}, lo={lo:.1f}, hi={hi:.1f})\")\n",
    "\n",
    "quantile = int((x > x.quantile(.995)).sum())\n",
    "print(f\"Extreme net_weight > p99.5: {quantile}\")\n",
    "\n",
    "# histogram\n",
    "y = pd.to_numeric(x, errors=\"coerce\")\n",
    "plt.figure()\n",
    "#plt.hist(x, bins=60)\n",
    "plt.hist(\n",
    "    y,\n",
    "    bins=\"fd\",                 \n",
    "    color=\"#89a978\",           \n",
    "    edgecolor=\"white\",         \n",
    "    alpha=0.95\n",
    ")\n",
    "plt.title(\"Receivals: net_weight\", weight=\"bold\")\n",
    "plt.xlabel(\"net_weight\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(\n",
    "    y,\n",
    "    bins=\"fd\",\n",
    "    color=\"#87CEEB\",\n",
    "    edgecolor=\"white\",\n",
    "    alpha=0.95,\n",
    "    range=(28000, 32000)     \n",
    ")\n",
    "plt.xlim(28000, 32000)\n",
    "plt.title(\"Receivals: net_weight – right tale\", weight=\"bold\")\n",
    "plt.xlabel(\"net_weight\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "x = receivals[\"net_weight\"]\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(x, '.', alpha=0.5)\n",
    "plt.title(\"Index vs Value — net_weight\", weight = \"bold\")\n",
    "plt.xlabel(\"Row index\")\n",
    "plt.ylabel(\"net_weight\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b147eb",
   "metadata": {},
   "source": [
    "Should exclude missing net_weights exclude when summing. Large peak around 2000 and 24000/25000. The distribution is bimodal.\n",
    "Outliers: a short right tail up to around 29k, few extreme values. With 0.5×IQR fences, hi = q3 + 0.5·IQR ≈ 21,110 + 7,730 ≈ 28,840, only 10 outliers over that.\n",
    "\n",
    "There’s no visible trend or step-change across the index: the cloud looks the same from row 0 to row 120k. That means nothing suspicious about file ordering (no block of weird zeros, no sudden shift).\n",
    "A few very small weights exist, but they’re rare, no huge spikes beyond ~30k."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe607832",
   "metadata": {},
   "source": [
    "####  date_arrival, receival_status, rm_id and supplier_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70b180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "recv5 = receivals.copy()\n",
    "import shutil\n",
    "\n",
    "ds = recv5[\"date_arrival\"]\n",
    "if len(ds):\n",
    "    print(f\"min  {ds.min()} | max = {ds.max()}\")\n",
    "    print(f\"Rows with date = {len(ds):,} / {len(recv):,}\")\n",
    "\n",
    "n = \"Receival status distribution\"\n",
    "print(f\"\\n{n}\")\n",
    "print(\"-\" * (len(n) + 20))\n",
    "rs = recv[\"receival_status\"]\n",
    "vc = rs.astype(\"object\").fillna(\"(NA)\").value_counts()\n",
    "print(vc.to_string())\n",
    "\n",
    "rm = recv5[\"rm_id\"].astype(\"object\").fillna(\"(NA)\").value_counts().head(20)\n",
    "sp = recv5[\"supplier_id\"].astype(\"object\").fillna(\"(NA)\").value_counts().head(20)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(18, 5), constrained_layout=True, sharey=True)\n",
    "\n",
    "rm.plot(kind=\"bar\", ax=axs[0], color=\"#ff7f0e\", edgecolor=\"white\")\n",
    "axs[0].set_title(\"Receivals: rm_id (top 20)\", weight=\"bold\")\n",
    "axs[0].set_xlabel(\"rm_id\")\n",
    "axs[0].set_ylabel(\"count\")\n",
    "axs[0].tick_params(axis=\"x\", labelrotation=90)\n",
    "\n",
    "sp.plot(kind=\"bar\", ax=axs[1], color=\"#ff7f0e\", edgecolor=\"white\")\n",
    "axs[1].set_title(\"Receivals: supplier_id (top 20)\", weight=\"bold\")\n",
    "axs[1].set_xlabel(\"supplier_id\")\n",
    "axs[1].tick_params(axis=\"x\", labelrotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dd9535",
   "metadata": {},
   "source": [
    "### Purchase orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dc8efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prof_po = preview_feature_summary(po, \"purchase_orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e650148",
   "metadata": {},
   "source": [
    "Quantities: handle negative and extreme values, confirm business meaning (returns, cancellations, corrections).\n",
    "\n",
    "Both KG and PUND is used, should standardize to KG before linking to receivals/weights.\n",
    "\n",
    "Filters for analyses: typically restrict to status in {\"Open\",\"Closed\"} and positive quantities unless you’re explicitly analyzing corrections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cfde33",
   "metadata": {},
   "source": [
    "#### Quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bcf180",
   "metadata": {},
   "outputs": [],
   "source": [
    "po4 = purchase_orders.copy()\n",
    "\n",
    "suspect = []\n",
    "neg_mask   = po4[\"quantity\"] < 0\n",
    "zero_mask  = (po4[\"quantity\"] == 0)\n",
    "\n",
    "quantity = po4[\"quantity\"]\n",
    "Q1, Q3 = quantity.quantile([.25, .75])\n",
    "IQR = Q3 - Q1\n",
    "high = q3 + 1.5*IQR \n",
    "iqr_mask  = (po4[\"quantity\"] > high)\n",
    "\n",
    "suspect.extend([neg_mask, zero_mask, iqr_mask])\n",
    "\n",
    "print(f\"Rows: {len(po4):,}\")\n",
    "print(f\"Q1={Q1:.0f}, Q3={Q3:.0f}, IQR={IQR:.0f}  |  cutoffs: hi={high:.0f}\")\n",
    "print(f\"Negatives: {neg_mask.sum():,}\")\n",
    "print(f\"Zeros: {zero_mask.sum():,}\")\n",
    "print(f\"IQR outliers: {iqr_mask.sum():,}\")\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.boxplot(quantity, vert=True, showfliers=True)\n",
    "plt.title(\"Purchase order quantity\", weight=\"bold\")\n",
    "plt.ylabel(\"quantity\")\n",
    "plt.axhline(high, linestyle=\"--\", linewidth=1)\n",
    "plt.text(1.05, high, f\"high cutoff ≈ {high:,.0f}\", va=\"center\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.boxplot(quantity, vert=True, showfliers=True)\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Purchase order quantity (log scale)\", weight=\"bold\")\n",
    "plt.ylabel(\"quantity (log scale)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08913557",
   "metadata": {},
   "source": [
    "The “middle” 50% of orders sits between 10k and 100k units.\n",
    "\n",
    "Negatives: 6 rows.\n",
    "\n",
    "Zeros: 4 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b14d81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = po4[\"delivery_date\"]\n",
    "if len(ds):\n",
    "    print(f\"min  {ds.min()} | max = {ds.max()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578c3c98",
   "metadata": {},
   "source": [
    "Different coverage windows. The purchase order table goes back to 2002-01-30, while receivals start at 2004-06-15. If the warehouse/receivals system (or your extract) only has data from mid-2004 onward, any purchase order delivery dates before that will naturally have no matching receipts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9194d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"Purchase order status\"\n",
    "print(f\"\\n{t}\")\n",
    "print(\"-\" * (len(t) + 20))\n",
    "status = po4[\"status\"]\n",
    "vc = status.astype(\"object\").fillna(\"(NA)\").value_counts()\n",
    "print(vc.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c405d3",
   "metadata": {},
   "source": [
    "### Transportation and materials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98373f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = materials.copy()\n",
    "trans = transportation.copy()\n",
    "preview_feature_summary(mat, \"materials\")\n",
    "preview_feature_summary(trans, \"transportation\", top_n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdb4091",
   "metadata": {},
   "source": [
    "### Weight over months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4524c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = receivals.copy()\n",
    "df[\"date_arrival\"] = pd.to_datetime(df[\"date_arrival\"], utc=True, errors=\"coerce\")\n",
    "df[\"net_weight\"]   = pd.to_numeric(df[\"net_weight\"], errors=\"coerce\")\n",
    "\n",
    "monthly = (\n",
    "    df.set_index(\"date_arrival\")\n",
    "      .resample(\"MS\")[\"net_weight\"]\n",
    "      .sum(min_count=1)\n",
    ")\n",
    "\n",
    "roll3 = monthly.rolling(3, min_periods=1).mean()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(monthly.index, monthly.values, label=\"Monthly total\")\n",
    "plt.plot(roll3.index, roll3.values, linestyle=\"--\", label=\"3-month avg\")\n",
    "plt.title(\"Total net_weight per month\", weight = \"bold\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"net_weight\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8239d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = receivals.copy()\n",
    "m[\"date_arrival\"] = pd.to_datetime(m[\"date_arrival\"], utc=True, errors=\"coerce\")\n",
    "m[\"net_weight\"] = pd.to_numeric(m[\"net_weight\"], errors=\"coerce\")\n",
    "\n",
    "by_moy_avg = (\n",
    "    m.set_index(\"date_arrival\")[\"net_weight\"]\n",
    "     .resample(\"MS\").sum()\n",
    "     .groupby(lambda d: d.month).mean()\n",
    "     .reindex(range(1,13))\n",
    ")\n",
    "\n",
    "month_names = [pd.Timestamp(2000, m, 1).strftime(\"%b\") for m in range(1,13)]\n",
    "plt.bar(month_names, by_moy_avg.values, color=\"#89a978\")\n",
    "plt.title(\"Average monthly total net_weight (across all years)\", weight = \"bold\")\n",
    "plt.xlabel(\"Month\"); plt.ylabel(\"Average total net_weight\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76f708a",
   "metadata": {},
   "source": [
    "## 5. Explore Pairs and Groups of Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f64ffe",
   "metadata": {},
   "source": [
    "\n",
    "- Ordered vs Received (before/after timing-fair) with side-by-side tables\n",
    "- Split shipments vs completion time (+ under-fulfil by split bin)\n",
    "- Shipment size CV (materials, top suppliers)\n",
    "- Supplier reliability (cleaned delays)\n",
    "- Correlation heatmap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f759cb1e",
   "metadata": {},
   "source": [
    "### 1. Parse dates & join → rec_po"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e86f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (9, 4)\n",
    "plt.rcParams.update({\n",
    "    \"axes.titlesize\": 12,   \n",
    "    \"axes.labelsize\": 10,   \n",
    "    \"xtick.labelsize\": 9,  \n",
    "    \"ytick.labelsize\": 9,\n",
    "    \"legend.fontsize\": 8,\n",
    "})\n",
    "\n",
    "receivals = receivals[receivals[\"net_weight\"] > 0].copy()\n",
    "\n",
    "rec_po = receivals.merge(\n",
    "    purchase_orders[[\n",
    "        \"purchase_order_id\",\"purchase_order_item_no\",\"quantity\",\"delivery_date\",\"product_id\",\"product_version\"\n",
    "    ]],\n",
    "    on=[\"purchase_order_id\",\"purchase_order_item_no\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "print(\"rec_po rows:\", len(rec_po))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d78d22",
   "metadata": {},
   "source": [
    "### 4. Placeholder cleanup → rec_base & rec_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e168bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make tz-naive day fields\n",
    "rec_base = rec_po.copy()\n",
    "rec_base[\"arrival_day\"]  = rec_base[\"date_arrival\"].dt.tz_convert(None).dt.floor(\"D\")\n",
    "rec_base[\"delivery_day\"] = rec_base[\"delivery_date\"].dt.tz_convert(None).dt.floor(\"D\")\n",
    "\n",
    "# Flag placeholders: end-of-month expected dates (includes Dec 31 / year-end)\n",
    "rec_base[\"placeholder\"] = rec_base[\"delivery_day\"].dt.is_month_end\n",
    "\n",
    "# Raw delay for reference\n",
    "rec_base[\"delay_raw\"] = (rec_base[\"arrival_day\"] - rec_base[\"delivery_day\"]).dt.days\n",
    "\n",
    "# Cleaned view = remove placeholder rows for delay analysis\n",
    "rec_clean = rec_base[~rec_base[\"placeholder\"]].copy()\n",
    "rec_clean[\"delay_days\"] = rec_clean[\"delay_raw\"]  # canonical delay column for all delay plots\n",
    "\n",
    "print(\"Placeholder rate:\", rec_base[\"placeholder\"].mean())\n",
    "print(\"Median delay raw / clean:\", np.nanmedian(rec_base[\"delay_raw\"]), \"/\", np.nanmedian(rec_clean[\"delay_days\"]))\n",
    "print(\"On-time share raw / clean:\", (rec_base[\"delay_raw\"]<=0).mean(), \"/\", (rec_clean[\"delay_days\"]<=0).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0539d8c0",
   "metadata": {},
   "source": [
    "### 5. Ordered vs. Received "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b646bb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-material totals \n",
    "rm_received = (rec_po.groupby(\"rm_id\", as_index=False)[\"net_weight\"]\n",
    "                   .sum()\n",
    "                   .rename(columns={\"net_weight\":\"rm_total_received\"}))\n",
    "\n",
    "rm_unique_po = rec_po.drop_duplicates([\"rm_id\",\"purchase_order_id\",\"purchase_order_item_no\"])\n",
    "rm_ordered   = (rm_unique_po.groupby(\"rm_id\", as_index=False)[\"quantity\"]\n",
    "                           .sum()\n",
    "                           .rename(columns={\"quantity\":\"rm_total_ordered\"}))\n",
    "\n",
    "rm_level = (rm_received.merge(rm_ordered, on=\"rm_id\", how=\"outer\")\n",
    "                      .fillna(0))\n",
    "\n",
    "rm_level[\"rm_fulfilment_ratio\"] = np.where(\n",
    "    rm_level[\"rm_total_ordered\"] > 0,\n",
    "    rm_level[\"rm_total_received\"] / rm_level[\"rm_total_ordered\"],\n",
    "    np.nan\n",
    ")\n",
    "rm_level[\"gap\"] = rm_level[\"rm_total_ordered\"] - rm_level[\"rm_total_received\"]\n",
    "\n",
    "# Scatter: ordered vs received (per material)\n",
    "x = rm_level[\"rm_total_ordered\"].to_numpy()\n",
    "y = rm_level[\"rm_total_received\"].to_numpy()\n",
    "\n",
    "ax = rm_level.plot.scatter(x=\"rm_total_ordered\", y=\"rm_total_received\", alpha=0.6, color=\"skyblue\")\n",
    "lims = [\n",
    "    min(ax.get_xlim()[0], ax.get_ylim()[0]),\n",
    "    max(ax.get_xlim()[1], ax.get_ylim()[1]),\n",
    "]\n",
    "ax.plot(lims, lims, ls=\"--\", lw=1, color=\"gray\")\n",
    "ax.set_xlim(lims); ax.set_ylim(lims)\n",
    "ax.set_title(\"Per material: Total ordered vs total received\")\n",
    "ax.set_xlabel(\"Total ordered quantity\"); ax.set_ylabel(\"Total received (net weight)\")\n",
    "plt.grid(True, alpha=0.3); plt.show()\n",
    "\n",
    "# Worst fulfilment ratios \n",
    "TOP_N = 10\n",
    "MIN_ORDER = 1_000_000\n",
    "\n",
    "worst_ratio = (rm_level.loc[rm_level[\"rm_total_ordered\"] >= MIN_ORDER,\n",
    "                            [\"rm_id\",\"rm_total_ordered\",\"rm_total_received\",\"rm_fulfilment_ratio\"]]\n",
    "                        .rename(columns={\"rm_fulfilment_ratio\":\"ratio\"})\n",
    "                        .sort_values(\"ratio\", na_position=\"last\")\n",
    "                        .head(TOP_N))\n",
    "\n",
    "worst_ratio[\"ratio\"] = worst_ratio[\"ratio\"].round(3)\n",
    "\n",
    "print(f\"Worst fulfilment ratios (Top {TOP_N}, min order ≥ {MIN_ORDER:,}):\")\n",
    "print(worst_ratio.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fd0386",
   "metadata": {},
   "source": [
    "The scatter compares total ordered (x) vs. total received (y) per material; the dashed 45° line marks perfect fulfilment. Most materials are low-volume near the origin, while several high-volume materials fall below the line, indicating notable under-fulfilment. This may reflect true shortfalls or simply timing (orders not yet due/received within our window). \n",
    "\n",
    "From the table, the three worst performers are rm_id 387, 381, and 383."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae93e17",
   "metadata": {},
   "source": [
    "### 6. Split-shipments vs completion time (+ under-fulfil by split bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6bb957",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = rec_po.copy()\n",
    "rec[\"arrival_day\"]  = rec[\"date_arrival\"].dt.tz_convert(None).dt.floor(\"D\")\n",
    "rec[\"delivery_day\"] = rec[\"delivery_date\"].dt.tz_convert(None).dt.floor(\"D\")\n",
    "\n",
    "po_qty = (rec.drop_duplicates([\"purchase_order_id\",\"purchase_order_item_no\"])\n",
    "            .set_index([\"purchase_order_id\",\"purchase_order_item_no\"])[\"quantity\"])\n",
    "\n",
    "g = rec.sort_values(\"arrival_day\").groupby([\"purchase_order_id\",\"purchase_order_item_no\"])\n",
    "n_recv = g.size().rename(\"n_receivals\")\n",
    "\n",
    "rec_seq = (rec.sort_values(\"arrival_day\")\n",
    "             .groupby([\"purchase_order_id\",\"purchase_order_item_no\",\"arrival_day\"])[\"net_weight\"]\n",
    "             .sum().groupby(level=[0,1]).cumsum().rename(\"cum_received\").reset_index())\n",
    "\n",
    "first_last = g[\"arrival_day\"].agg(first_arrival=\"min\", last_arrival=\"max\")\n",
    "\n",
    "def completion_day_for_item(df):\n",
    "    key = (df[\"purchase_order_id\"].iloc[0], df[\"purchase_order_item_no\"].iloc[0])\n",
    "    ord_qty = po_qty.loc[key] if key in po_qty.index else np.nan\n",
    "    if pd.isna(ord_qty):\n",
    "        return pd.NaT\n",
    "    reached = df[df[\"cum_received\"] >= ord_qty]\n",
    "    return reached[\"arrival_day\"].iloc[0] if not reached.empty else df[\"arrival_day\"].iloc[-1]\n",
    "\n",
    "comp = (rec_seq.groupby([\"purchase_order_id\",\"purchase_order_item_no\"])\n",
    "              .apply(completion_day_for_item).rename(\"completion_day\").reset_index())\n",
    "\n",
    "po_item_panel = (first_last\n",
    "                 .merge(n_recv, left_index=True, right_index=True)\n",
    "                 .reset_index()\n",
    "                 .merge(comp, on=[\"purchase_order_id\",\"purchase_order_item_no\"], how=\"left\")\n",
    "                 .merge(po_qty.rename(\"ordered\"), left_on=[\"purchase_order_id\",\"purchase_order_item_no\"], right_index=True, how=\"left\"))\n",
    "\n",
    "po_item_panel[\"completion_time_days\"] = (po_item_panel[\"completion_day\"] - po_item_panel[\"first_arrival\"]).dt.days\n",
    "\n",
    "po_item_panel[\"split_bin\"] = pd.cut(\n",
    "    po_item_panel[\"n_receivals\"],\n",
    "    bins=[0,1,3,7,1e9],\n",
    "    labels=[\"1\",\"2–3\",\"4–7\",\"8+\"],\n",
    "    right=True\n",
    ")\n",
    "\n",
    "final_received = (rec.groupby([\"purchase_order_id\",\"purchase_order_item_no\"])[\"net_weight\"].sum().rename(\"received_final\"))\n",
    "po_item_panel = po_item_panel.merge(final_received, on=[\"purchase_order_id\",\"purchase_order_item_no\"], how=\"left\")\n",
    "po_item_panel[\"under_fulfil\"] = po_item_panel[\"received_final\"] < po_item_panel[\"ordered\"]\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "sns.boxplot(data=po_item_panel.dropna(subset=[\"split_bin\",\"completion_time_days\"]),\n",
    "            x=\"split_bin\", y=\"completion_time_days\", fliersize=1, color=\"skyblue\")\n",
    "plt.title(\"Completion time by # of receivals per PO item\")\n",
    "plt.xlabel(\"# receivals bin\"); plt.ylabel(\"Completion time (days)\")\n",
    "plt.grid(True, axis=\"y\", alpha=0.3); plt.show()\n",
    "\n",
    "uf = (po_item_panel.groupby(\"split_bin\")[\"under_fulfil\"]\n",
    "      .mean().rename(\"under_fulfil_share\").reset_index())\n",
    "display(uf.style.format({\"under_fulfil_share\":\"{:.2f}\"}).set_caption(\"Under-fulfil share by split bin\"))\n",
    "\n",
    "plot_df = po_item_panel.dropna(subset=[\"split_bin\",\"completion_time_days\"]).query(\"completion_time_days > 0\")\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "sns.boxplot(data=plot_df, x=\"split_bin\", y=\"completion_time_days\", fliersize=1, color=\"skyblue\")\n",
    "plt.yscale(\"log\")  # <- log scale\n",
    "plt.title(\"Completion time by # of receivals per PO item\")\n",
    "plt.xlabel(\"# receivals bin\"); plt.ylabel(\"Completion time (days, log)\")\n",
    "plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1e4b10",
   "metadata": {},
   "source": [
    "More splits → slower completion. PO items delivered in one receival finish fastest. As the number of receivals grows (2–3 → 4–7 → 8+), the median completion time increases and the tail gets much longer (some items take many months). \n",
    "\n",
    "Implication: items split into many parts (or oddly, single-drop items) are riskier; they take longer and are more likely to end below the ordered quantity. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef77673",
   "metadata": {},
   "source": [
    "### 7. Shipment-size CV (materials & top suppliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f8d005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Materials\n",
    "rm_stats = (rec_po.groupby(\"rm_id\")[\"net_weight\"].agg(n=\"count\", mean=\"mean\", std=\"std\")).reset_index()\n",
    "rm_stats[\"cv\"] = rm_stats[\"std\"] / rm_stats[\"mean\"]\n",
    "rm_stats = rm_stats.replace([np.inf,-np.inf], np.nan)\n",
    "rm_stats_filt = rm_stats[rm_stats[\"n\"] >= 10].sort_values(\"cv\", ascending=False)\n",
    "\n",
    "display(rm_stats_filt.head(15).style.format({\"mean\":\"{:,.0f}\", \"std\":\"{:,.0f}\", \"cv\":\"{:.2f}\", \"n\":\"{:,.0f}\"})\n",
    "        .set_caption(\"Highest shipment-size CV (materials, n≥10)\"))\n",
    "\n",
    "plt.figure(figsize=(9,4))\n",
    "sns.histplot(rm_stats_filt[\"cv\"].dropna(), bins=40, color=\"skyblue\")\n",
    "plt.title(\"Distribution of shipment-size CV across materials (n≥10)\")\n",
    "plt.xlabel(\"CV of receival net_weight\"); plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Suppliers (top by delivered volume)\n",
    "topN = 15\n",
    "top_suppliers = (rec_po.groupby(\"supplier_id\")[\"net_weight\"].sum()\n",
    "                 .sort_values(ascending=False).head(topN).index)\n",
    "\n",
    "sup_stats_cv = (rec_po[rec_po[\"supplier_id\"].isin(top_suppliers)]\n",
    "                .groupby(\"supplier_id\")[\"net_weight\"]\n",
    "                .agg(n=\"count\", mean=\"mean\", std=\"std\")).reset_index()\n",
    "sup_stats_cv[\"cv\"] = sup_stats_cv[\"std\"] / sup_stats_cv[\"mean\"]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(data=sup_stats_cv.sort_values(\"cv\", ascending=False),\n",
    "            x=\"supplier_id\", y=\"cv\", color=\"skyblue\")\n",
    "plt.title(\"Shipment-size CV by supplier (top by volume)\")\n",
    "plt.xlabel(\"Supplier\"); plt.ylabel(\"CV\")\n",
    "plt.xticks(rotation=45, ha=\"right\"); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9e9cb3",
   "metadata": {},
   "source": [
    "Top-CV materials (table): some materials have CV ≈ 1.0–1.5 (e.g., rm_id ~1981, 2283, 3421). That means their shipment-size std is about the same as, or larger than, the mean: very inconsistent lot sizes.\n",
    "\n",
    "Histogram (materials, n≥10): most materials sit around CV 0.3–0.7, with a long tail above 0.8–1.0. A practical rule: CV > 0.8 = high variability (riskier to forecast).\n",
    "\n",
    "Supplier bar chart (top by volume): suppliers differ a lot. Some are stable (CV ≈ 0.05–0.25), while others are volatile (CV ≈ 0.7–0.95). High-CV suppliers will make inflow lumpier and should push forecasts to be more conservative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2db3eb8",
   "metadata": {},
   "source": [
    "### 3. Supplier reliability (using cleaned delays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136086b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sup = rec_clean.copy()\n",
    "\n",
    "# PO-item fulfilment \n",
    "po_item = (sup.groupby([\"purchase_order_id\",\"purchase_order_item_no\"])\n",
    "             .agg(ordered=(\"quantity\",\"first\"), received=(\"net_weight\",\"sum\")).reset_index())\n",
    "po_item[\"fulfil\"] = po_item[\"received\"]/po_item[\"ordered\"]\n",
    "sup = sup.merge(po_item, on=[\"purchase_order_id\",\"purchase_order_item_no\"], how=\"left\")\n",
    "\n",
    "sup_stats = (sup.groupby(\"supplier_id\")\n",
    "  .agg(n=(\"net_weight\",\"count\"),\n",
    "       on_time=(\"delay_days\", lambda s: (s<=0).mean()),\n",
    "       med_delay=(\"delay_days\",\"median\"),\n",
    "       under_fulfil=(\"fulfil\", lambda s: (s<1).mean()))\n",
    "  .reset_index())\n",
    "\n",
    "topN = 20\n",
    "top_suppliers = (sup.groupby(\"supplier_id\")[\"net_weight\"].sum()\n",
    "                 .sort_values(ascending=False).head(topN).index)\n",
    "sup_top = sup[sup[\"supplier_id\"].isin(top_suppliers)]\n",
    "sup_stats_top = sup_stats[sup_stats[\"supplier_id\"].isin(top_suppliers)]\n",
    "sup_top[\"delay_days_clipped\"] = sup_top[\"delay_days\"].clip(lower=-30, upper=120)\n",
    "\n",
    "# Delay boxplot\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(data=sup_top, x=\"supplier_id\", y=\"delay_days_clipped\", fliersize=1, color=\"skyblue\")\n",
    "plt.axhline(0, ls=\"--\", lw=1, color=\"gray\")\n",
    "plt.title(\"Delay boxplot by supplier (top by volume)\")\n",
    "plt.xlabel(\"Supplier\"); plt.ylabel(\"Delay (days, clipped)\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# On-time share bar\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(data=sup_stats_top.sort_values(\"on_time\", ascending=False),\n",
    "            x=\"supplier_id\", y=\"on_time\", color=\"skyblue\")\n",
    "plt.title(\"On-time share by supplier (cleaned)\")\n",
    "plt.xlabel(\"Supplier\"); plt.ylabel(\"On-time share\")\n",
    "plt.ylim(0,1); plt.xticks(rotation=45, ha=\"right\"); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e90410",
   "metadata": {},
   "source": [
    "Most top suppliers deliver earlier than the expected date (medians below 0), but many still have occasional big delays (the tall outliers). A few suppliers have medians around or above 0 and much wider spreads, these look less reliable and are more likely to be late. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de380471",
   "metadata": {},
   "source": [
    "### 9. Correlation heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2100264",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.triu(np.ones_like(corr, dtype=bool), k=1)\n",
    "\n",
    "sns.set_context(\"notebook\", font_scale=0.9)\n",
    "fig, ax = plt.subplots(figsize=(7, 6), dpi=120)\n",
    "\n",
    "hm = sns.heatmap(\n",
    "    corr,\n",
    "    mask=mask,\n",
    "    cmap=\"RdBu_r\",        # diverging, centered at 0\n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    annot=True, fmt=\".2f\",\n",
    "    annot_kws={\"size\":11},\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\":0.8, \"pad\":0.02}\n",
    ")\n",
    "\n",
    "ax.set_title(\"Correlation heatmap (per-material features)\", pad=12)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "\n",
    "# Make annotation text switch color for contrast\n",
    "for txt in ax.texts:\n",
    "    try:\n",
    "        val = float(txt.get_text())\n",
    "    except ValueError:\n",
    "        continue\n",
    "    txt.set_color(\"white\" if abs(val) > 0.6 else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8943d6d4",
   "metadata": {},
   "source": [
    "Total received is driven mostly by volume proxies: the number of receivals has the strongest association with received (very high), and total ordered also correlates strongly. Materials sourced from more suppliers tend to receive more overall (moderate–strong link). In contrast, shipment-size statistics (mean/median weight per receival) and size variability (CV) show only weak relationships with total received, and delay is essentially unrelated to the final quantity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d61fd5",
   "metadata": {},
   "source": [
    "## 6. Clean Up Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8422bdba",
   "metadata": {},
   "source": [
    "\n",
    "Each dataset (`receivals.csv` and `purchase_orders.csv`) was cleaned individually to remove incorrect or physically impossible values, standardize units, and ensure temporal and ID consistency, following the procedures outlined in section 2.5. \n",
    "\n",
    "The cleaned datasets were then merged into a single dataset (`training_data.csv`) containing only valid receivals with positive target values (`net_weight`) and quantities. \n",
    "\n",
    "For modeling purposes, only the relevant columns (`rm_id`, `date_arrival`, `net_weight`, `quantity`) were retained. The data was further aggregated by `rm_id` and `date_arrival_day` to combine multiple receivals on the same day, producing `training_data_aggregated.csv`, which is sorted by `rm_id` and `date_arrival`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd40a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "receivals = receivals.drop_duplicates()\n",
    "purchase_orders = purchase_orders.drop_duplicates()\n",
    "\n",
    "# Remove physically impossible values\n",
    "receivals = receivals[receivals[\"net_weight\"] > 0]\n",
    "purchase_orders = purchase_orders[purchase_orders[\"quantity\"] > 0]\n",
    "\n",
    "# Standardize units to kilograms\n",
    "if \"unit\" in purchase_orders.columns:\n",
    "    purchase_orders[\"unit\"] = purchase_orders[\"unit\"].str.lower()\n",
    "    purchase_orders.loc[purchase_orders[\"unit\"].isin([\"pund\", \"lbs\", \"pound\"]), \"quantity\"] *= 0.45359237\n",
    "    purchase_orders[\"unit\"] = \"kg\"\n",
    "\n",
    "# Ensure temporal consistency (receivals should not happen before order creation)\n",
    "merged_temp = receivals.merge(\n",
    "    purchase_orders[[\"purchase_order_id\", \"purchase_order_item_no\", \"created_date_time\"]],\n",
    "    on=[\"purchase_order_id\", \"purchase_order_item_no\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "receivals = merged_temp[merged_temp[\"date_arrival\"] >= merged_temp[\"created_date_time\"]].copy()\n",
    "receivals.drop(columns=\"created_date_time\", inplace=True)\n",
    "\n",
    "# Keep only valid IDs\n",
    "receivals = receivals[receivals[\"purchase_order_id\"] > 0]\n",
    "purchase_orders = purchase_orders[purchase_orders[\"purchase_order_id\"] > 0]\n",
    "\n",
    "# Merge datasets\n",
    "training_data = pd.merge(\n",
    "    receivals,\n",
    "    purchase_orders,\n",
    "    on=[\"purchase_order_id\", \"purchase_order_item_no\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Save final cleaned dataset\n",
    "training_data.to_csv(f\"{data_path}/training_data.csv\", index=False)\n",
    "print(f\"Training_data.csv created successfully with {len(training_data):,} rows.\")\n",
    "\n",
    "# Keep only the relevant columns for modeling\n",
    "training_data = training_data[['rm_id', 'date_arrival', 'net_weight', 'quantity']]\n",
    "\n",
    "# Floor date to day for aggregation\n",
    "training_data['date_arrival'] = training_data['date_arrival'].dt.floor('D')\n",
    "\n",
    "# Aggregate data by material and day\n",
    "training_data_agg = training_data.groupby(['rm_id', 'date_arrival'], as_index=False).agg(\n",
    "    net_weight=('net_weight', 'sum'),\n",
    "    quantity=('quantity', 'first') # TODO: Check later\n",
    ")\n",
    "training_data_agg = training_data_agg.sort_values(['rm_id', 'date_arrival'])\n",
    "\n",
    "# Save aggregated dataset\n",
    "training_data_agg.to_csv(f\"{data_path}/training_data_aggregated.csv\", index=False)\n",
    "print(f\"Training_data_aggregated.csv created successfully with {len(training_data_agg):,} rows.\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_summary = training_data_agg.isnull().sum()\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7bd93d",
   "metadata": {},
   "source": [
    "# **Feature Engineering**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
